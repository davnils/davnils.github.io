<?xml version="1.0" encoding="UTF-8" ?>

<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
    <head>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
        <title>blog :: Set Thought → IO ∅</title>
        <link rel="stylesheet" type="text/css" href="./css/default.css" />

        <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/javascript">
        MathJax.Hub.Config({
          tex2jax: {
            inlineMath: [ ['$','$'], ['\\(','\\)'] ]
          }
        });
        </script>
    </head>
    <body>
        <div id="header">
            <span id="title">blog :: Set Thought → IO ∅</span>
            <span id="menu">
                <a href="./">Blog</a>
                <a href="https://github.com/davnils" target="_blank">Code</a>
                <a href="./about.html">About</a>
            </span>
        </div>

        <div id="content">
            
  <a id="distributed_btc_markets"></a>
  <div class="post">
<h2><a href="#distributed_btc_markets" id="postTitle">Distributed approach to BTC markets</a></h2>

<div class="info">
  Posted on October 23, 2013
</div>

  <p>This post will describe a recent project of mine; fetching bitcoin market data using Haskell. I will give a brief introduction to the problem being solved, the solution, and describe what benefits Haskell provides.</p>
<h4 id="introduction">Introduction</h4>
<p>Bitcoin markets typically offer some public API built around the JSON format. Through HTTP requests you can retrieve the current order book and complete trade history. An example of existing formats is a standardized version established by <a href="http://bitcoincharts.com/about/exchanges/">bitcoincharts</a>. A simple fetching program could simply poll each market repeatedly and dump the results, maintaining some state related to the latest retrieved trade. There are a few issues with this simplistic approach, since it implicitly relies on requesting data from a single node and some database backend is probably needed as well. Another take on this is splitting up the fetching across several nodes and managing the role of each node. Such a distributed system allows lower dependence on single nodes and also makes scaling a lot easier. There is also the problem of having assurance in your implementation. Haskell is a big part of the solution, since all components of the program can be expressed on an abstract level, and the type system allows imposing restrictions on a subset of the program.</p>
<h4 id="distributed-approach">Distributed approach</h4>
<p>Consider the following sketch of a system where the Cassandra database storage, a single backend node, a proxy layer, and third party markets are split up into layers.</p>
<p><img src="images/market-overview.png" /></p>
<p>The backend is responsible of managing all active markets and basically parses a configuration file for each market. It maintains state related to when a given market should be polled next. Proxy nodes connect into the backend and are managed using separate threads, which then communicate with application logic using channels. Communication with the proxy layer is abstracted into a monad transformer which is used by a set of functions. This allows managing proxy nodes, market requests, and retransmissions by simply calling some high level function.</p>
<p>The proxy layer is basically a collection of simple nodes pooled together. The goal is to isolate public-facing communication into these proxy nodes and keep a clean interface towards the backend node. When the backend decides that some market state needs to be updated it transmits a request to some randomly chosen proxy node. The proxy nodes parses the included configuration and deals with HTTP specifics upon connecting to the given market. The response is then serialized into a binary blob and sent back to the backend. Upon receiving this response all processing related to order books and trades kick in, and results are pushed into a Cassandra cluster (which is a distributed database).</p>
<p>Layering all logic in this way allows easy abstraction of features and also handles assurance in implementations. For example all communication between the backend and proxy nodes is over a well-defined protocol and HTTP-related connections (with all the related complexity) are strictly isolated to proxy nodes. Hence unexpected crashes and broken web servers are problems isolated to proxy nodes, and the backend can trivially issue retransmissions and manage the active set of nodes in the background. There is also the benefit of requesting data from a larger set of IP addresses. If a node would ever by IP-blocked (for whatever reason) or have regional connectivity issues it will simply be replaced by the backend until the issues are resolved. In the end the market data will be available if it is available to any node on the internet, given a large enough pool. Another benefit is that scaling the pool is trivial - just launch a new proxy and it will integrate into the system. Updates of proxy software can be applied partially with zero downtime as well.</p>
<p>Update (2013-10-28): This design is not optimal for all use cases, since the backend node is a single point of failure and potential bottleneck. In this case the simple distributed alternative does offer some benefits while not dealing with problems such as distributed state.</p>
<h4 id="haskell-libraries">Haskell libraries</h4>
<p>An important part of this project is the wonderful ecosystem of Haskell; here are some highlights.</p>
<p><a href="http://hackage.haskell.org/package/pipes">Pipes</a> provides construction of pipelines from reusable components. This pattern is heavily utilized throughout this project since many things can be expressed as pipelines, i.e. read some thing, process it, and save the output. Since version 4.0 it is actually possible to comprehend mixing monads within the pipeline, since pipes just adds a monad transformer upon your existing stack. It is truly beautiful. Pipes by itself is good thing, but more importantly there is a separate ecosystem implementing much common functionality in terms of pipes. For example you can easily setup a pipeline fetching an action from a shared STM variable, generate a response object, serialize it, and transmit the result through a socket.</p>
<p><a href="http://hackage.haskell.org/package/http-streams">http-streams</a> is an easy-to-use HTTP client which does what you want basically, and doesn’t bother you with specifics. There are however some issues with the underlying SSL dependency which uses FFI, e.g. dependency on compilation flags and segfaulting upon certain errors. It is also important to take great care of all exceptions that might occur, the <a href="http://hackage.haskell.org/package/errors">errors</a> package helps out a great deal here (for example see the syncIO function).</p>
<p><a href="http://hackage.haskell.org/package/aeson">Aeson</a> and <a href="http://hackage.haskell.org/package/cassandra-cql">cassandra-cql</a> are good examples of translating data through different representations and storing content. Typically a pattern emerges where you create a new type and then express serialization by composing existing instances. An example of this is working around limitations of storing pairs in Cassandra. The driver allows expressing custom serialization into a blob instance, working around existing limitations easily. It also has a great monadic interface when composing queries which are at some point executed on a pool of servers.</p>
<p><a href="http://hackage.haskell.org/package/snap">Snap</a> and <a href="http://hackage.haskell.org/package/QuickCheck">QuickCheck</a> makes testing a breeze. Specifically Snap is used to easily fire up a HTTP server, where each route shares a global communication channel with the actual test case. Using this it becomes easy to simulate a market and for example verify the number of requests transmitted to a specific route (e.g. polling frequency of the order book).</p>
<h4 id="notes">Notes</h4>
<p>The source code of the project is available <a href="https://github.com/davnils/distr-btc">here</a>.</p>

  <div class="box"></div>
</div>


  <a id="thoughts_on_service_fingerprinting"></a>
  <div class="post">
<h2><a href="#thoughts_on_service_fingerprinting" id="postTitle">Thoughts On Service Fingerprinting</a></h2>

<div class="info">
  Posted on August 18, 2013
</div>

  <p>There are a lot of services connected to the public internet and private networks. A crucial part of pentesting different networks involves learning about what software is running and also more detailed information such as versions and configurations. This allows more efficient testing by limiting the number of probable vulnerabilities, but also makes deeper pentesting feasible.</p>
<p>Obviously there are problems when services try to hide their true identity. This might be present in the case of HTTP servers, where the Server field describes the software name and version. In this case the field will typically indicate the correct version, but this simple heuristic will fail in any non-trivial cases, and does not really provide any useful information. Hence a high quality scanner, operating on some category of services, should be to reliably detect software without depending on these obvious fields. The term fingerprinting typically involves reliably detecting software by studying all available information.</p>
<p>Fundamentally there are different domains of information which can be studied. This post covers the <i>Data</i> and <i>Time</i> domain, both of which are applicable to varying degrees with different inherent properties. In both cases it is of interest to classify the software (and perhaps also other properties), by choosing the most likely model that fits the data. Hence you will typically need some reference data set from which models of software combinations can be constructed. Given a set of models, the next step is to gather all the possible information, and retrieve the most likely model.</p>
<p>The data domain is what I consider being basically an application-level packet capture of network traffic. For example this is the transaction of HTTP headers captured when making an HTTP request. The benefits of only utilising this information boils down to simplicity and generality. An obvious example of this is that the underlying network stack and transport path does not affect the information content. If fingerprinting is done in a passive manner (i.e. with a provided packet capture) it might not be optimal. In this case the session keys must be provided. But it is still very powerful in the most common case of active probes.</p>
<p>Classification in the data domain can be done with a range of different techniques. I will describe solving the problem using an artificial neural network (ANN) classifier built using standard primitives. Firstly there is the problem of converting a series of headers into a suitable representation that will be fed into the classifier. This example will only consider the header part, even though the response body will certainly contain useful information when issuing certain requests. The series of headers should be transformed into a fixed-dimensional (length) vector in some way. This vector can then be used as input into a <a href="http://en.wikipedia.org/wiki/Multilayer_perceptron">multilayer perceptron</a> (a standard ANN) and be trained using supervised learning (i.e. given data set with input and result). Beginning by stating that the first k headers should be preserved for learning (with some large enough k), this limits the maximum vector length. Each header is on the format “key: value” where “key” belongs to a set of possible keys and “value” may be an arbitrary string or integer. Converting these into our fixed dimensional space imposes some natural restrictions, i.e. take the k first bytes and quantize integer values into ranges of interest. There is also the question of ordering - should headers be sorted according to some predefined ordering? This will obviously limit classification based on ordering properties and should be used carefully.</p>
<p>Labeling all the keys (quantization) can be done by evaluating them over an <a href="http://en.wikipedia.org/wiki/Autoencoder">autoencoder</a> ANN. This is constructed as a three-layer perceptron where the input and output layers are identical when learning is complete. The inner layer is smaller and represents compressed versions of all the inputs. Hence when learning is complete (i.e. inputs and outputs match for all labels), the first two layers can be extracted without losing any information. The output given by the inner layer is then used to construct label representation in the header vector, which in turn is used as input in the classifier. Note that training is simply repeated with a smaller inner layer until the model can’t describe all of the information content anymore.</p>
<p>Given a data set with a set of headers and their corresponding reference services, all the inputs can be built. Typically the fixed dimensional vector will be chunked into fixed length pieces, each corresponding to a row in the complete header packet. Each header row is then labeled as discussed and paired with a fixed length value field. This is a one-time process (including the autoencoder step) and will produce a processed data set that is fit for further real classification, independent of the method being employed. In this case the multilayer perceptron can be trained using the backprop algorithm. The resulting classifier can then be used in practice by performing the “input processing” step for a data set of unknown classification, yielding the most likely software.</p>
<p>The time domain provides a different set of properties but is typically not as generally applicable as classification in the data domain. Lets view HTTP requests from a different perspective; essentially the client makes a request and receives a reply which can be measured as bytes over time, possibly with some meaning associated to chunks of bytes. It is reasonable to believe that different services might produce different amounts of content over time, after all such a concern is not really present in the mind of an average developer. Simply plotting the number of bytes retrieved over time will yield a graph which could be studied manually in order to discern any interesting properties of a specific service. Automatic classification is obviously also interesting in this domain. There has been work done on <a href="http://www.cs.ucr.edu/~eamonn/time_series_data/">time series classification</a> which will take the complete data series and perform a classification. Alternatively it is possible to view the underlying service as something stateful. Consider for example that a XYZ service might deliver data through a set of different stages, perhaps performing some initial throttling of bandwidth and loosening up later on. Such features can be detected by taking the observations and comparing them with a set of existing models, extracting the most likely one. In the more general case it is common to use Hidden Markov Models (<a href="http://en.wikipedia.org/wiki/Hidden_Markov_model">HMM</a>) which are capable of deducing facts about the internal states by simply performing a sequence of observations. Some very similar work has been done by <a href="http://events.ccc.de/congress/2011/Fahrplan/events/4732.en.html">Burschka</a> (originally presented at the sec-t conference) where encrypted Skype traffic was monitored and the time domain was classified in order to detect the most likely spoken phrases, without access to the underlying audio stream.</p>
<p>There are however some potential issues when dealing with time domain classification. Commonly such an attack would be performed across LANs in a protocol encapsulated with TCP/IP. This interferes with timing in several ways, due to things like IP fragmentation and TCP segmentation, tightly bonded with MSS and congestion control. Obviously there is also the problem of variance over long network paths. Reducing the variance requires relocating probes or working around the problem with a larger set of samples, potentially becoming infeasible in a practical context.</p>
<p>In the end it is probably not feasible to perform classification in the time domain across networks. It might however be practical in certain contexts, e.g. in the case of a reduced network stack or very low latency network links with deterministic stacks. The data domain provides a reasonable set of assumptions and could be used to perform reliable classification for a wide range of services, as long as there is no active interference beyond the trivial cases. Using supervised learning to build a classifier does however require plenty of effort in acquiring a reliable data set over a wide range of configurations. Perhaps this could be improved by performing unsupervised learning while labeling the corresponding output classes in some clever way.</p>

  <div class="box"></div>
</div>


  <a id="algebra_and_information_structure"></a>
  <div class="post">
<h2><a href="#algebra_and_information_structure" id="postTitle">Algebra and Information Structure</a></h2>

<div class="info">
  Posted on July  2, 2013
</div>

  <p>Today I am writing on how it’s possible to find structure in information, bundled with functions, and how this can be applied in practice. This is highly applicable to software development but also comes from an interesting theoretical background.</p>
<p>So, lets begin with information. What can be said about a data set sharing some properties? Is it possible to relate some common operations and make general statements? Lets say we define a data structure working over integers - can it be extended to arbitrary objects forming some property? It would be nice if we could write general algorithms and data structures, with the fundamental constraints in mind, and reuse them to the fullest possible extent.</p>
<p>Algebra is a mathematical field which discusses, amongst other things, sets and related functions. This allows us to encapsulate information by providing a set of rules that need to be satisfied, in order for the information and possible functions to belong to some abstraction. In this post a basic set of abstractions are described and some practical examples are given. These are bundled with corresponding definitions in Haskell code, but the reader does not need any previous knowledge of the language in order to grasp the essence of these constructs.</p>
<h4 id="algebra">Algebra</h4>
<p>Lets begin with some definitions required to discuss these topics. Given a binary function <span class="math"><em>f</em>(<em>a</em>, <em>a</em>)</span> (denoted inline <span class="math"> † </span>) over a set <span class="math"><em>S</em></span>, the following is of interest (for all <span class="math"><em>a</em>, <em>b</em>, <em>c</em> ∈ <em>S</em></span>):</p>
<ul>
<li>function associativity: <span class="math">(<em>a</em> † <em>b</em>) † <em>c</em> = <em>a</em> † (<em>b</em> † <em>c</em>)</span></li>
<li>identity element <span class="math"><em>e</em></span> of <span class="math"><em>S</em></span>: <span class="math"><em>a</em> † <em>e</em> = <em>e</em> † <em>a</em> = <em>a</em></span></li>
<li>inverse element <span class="math"><em>a</em><sup> − 1</sup> ∈ <em>S</em></span> of <span class="math"><em>a</em> ∈ <em>S</em></span>: <span class="math"><em>a</em> † <em>a</em><sup> − 1</sup> = <em>a</em><sup> − 1</sup> † <em>a</em> = <em>e</em></span></li>
</ul>
<p>Furthermore <span class="math"><em>f</em></span> is <span class="math"><em>c</em><em>l</em><em>o</em><em>s</em><em>e</em><em>d</em></span> over <span class="math"><em>S</em></span> if <span class="math"><em>f</em>(<em>a</em>, <em>b</em>) ∈ <em>S</em></span></p>
<p>Adding these properties will make the function and set more capable of expressing different algorithms. But it also constrains the possible implementations (e.g. identity might not always exists), hence it is interesting to choose the smallest possible subset of laws that need to be satisfied.</p>
<p>Lets begin with the basic structures and gradually add more constrains, while studying applications of each subset.</p>
<h5 id="magma">Magma</h5>
<p>The simplest possible definition that is covered is the <span class="math"><em>m</em><em>a</em><em>g</em><em>m</em><em>a</em></span>. It consist of a binary operation <span class="math"><em>f</em></span> and a set <span class="math"><em>S</em></span>. The only requirement is that <span class="math"><em>f</em></span> is closed over <span class="math"><em>S</em></span>, nothing more.</p>
<p>Of course this very simplistic definition does not yield any structure in the problem and anything more constrained will still be considered a valid magma. It can modeled in Haskell using a type class on the following format:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="kw">class</span> <span class="dt">Magma</span> a <span class="kw">where</span>
<span class="ot">  f ::</span> a <span class="ot">-&gt;</span> a <span class="ot">-&gt;</span> a</code></pre>
<p>Hence any implementation (declared with instance DataType where …) would implement the function. It can be found in the <a href="http://hackage.haskell.org/package/magma">magma package</a> available on hackage.</p>
<h5 id="semigroup">Semigroup</h5>
<p>Semigroups add the associativity property to the magma. The name is derived from being a generalized version of groups, which are described below. Associativity allows a lot more powerful manipulations, which is appearant in the corresponding hackage package <a href="http://hackage.haskell.org/package/semigroups">semigroups</a>.</p>
<p>Due to the relatively weak typing being applied here there is no difference to the magma-typeclass, except with the name being substituted. Hence associativity is implicit on the type level, at least in this article.</p>
<p><span class="math"><em>F</em><em>o</em><em>l</em><em>d</em><em>i</em><em>n</em><em>g</em></span> is the operation of transforming a finite set into a single scalar value. Typically this can be realised by simply applying <span class="math"><em>f</em></span> onto the set repeatedly. This follows since the set cardinality is reduced by one for each application. Singleton sets can be considered as already reduced or invalid, the same applies to <span class="math">$\varnothing$</span>.</p>
<p>Without associativity this must be done while maintaining order and evaluating each operation in some precise way. Associativity allows us to simply study the set of values and apply the reduction in some arbitrary order. For example it is easy to realise a parallel reduction in a tree-like fashion. Simply by taking pairs of elements from the original set and transforming each pair in parallel to a scalar, this can be repeated until the final value is obtained. The final time complexity becomes <span class="math">$O(\log \left\vert S \right\vert)$</span> given the assumption that constructing <span class="math">{∀ <em>a</em> ∈ <em>S</em>: <em>f</em>(<em>a</em>)}</span> is considered <span class="math"><em>O</em>(1)</span> time, e.g. with a large amount of cores.</p>
<p>A simple example of folding is summing up a set of numbers, or taking the product. In the semigroups package there is also a nice application of non-empty lists being viewed as semigroups.</p>
<h5 id="monoid">Monoid</h5>
<p>A <span class="math"><em>m</em><em>o</em><em>n</em><em>o</em><em>i</em><em>d</em></span> is a semigroup that is constrained by satsifying the identity property. It’s typically used in a lot of different constructs and is present in the standard Haskell library (<a href="http://www.haskell.org/ghc/docs/latest/html/libraries/base/Data-Monoid.html">the base package</a>). It can be modelled with the following type class, where pure denotes the identity element:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="kw">class</span> <span class="dt">Semigroup</span> a <span class="ot">=&gt;</span> <span class="dt">Monoid</span> a <span class="kw">where</span>
<span class="ot">  pure ::</span> a</code></pre>
<p>The identity property allows trivial folding in the general case since we can always treat singleton sets <span class="math">{<em>a</em>}</span> or <span class="math">$\varnothing$</span> by using the identity element, as <span class="math"><em>f</em>(<em>a</em>, <em>e</em>)</span> and <span class="math"><em>e</em></span> respectively.</p>
<p>Monoids are extensively used within several fields and form a heavily utilized Haskell idiom when writing generic code. An example instance is string-like structures which form a monoid as follows:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="kw">instance</span> <span class="dt">Semigroup</span> <span class="dt">String</span> <span class="kw">where</span>
  f <span class="fu">=</span> concat

<span class="kw">instance</span> <span class="dt">Monoid</span> <span class="dt">String</span> <span class="kw">where</span>
  pure <span class="fu">=</span> <span class="st">&quot;&quot;</span></code></pre>
<p>This allows easily swapping the underlying string representation if only occurrences of <span class="math"><em>f</em></span> are used when performing string concatenation. There are also more advanced applications such as <a href="http://izbicki.me/blog/hlearn-cross-validates-400x-faster-than-weka">statistics</a>.</p>
<h5 id="group">Group</h5>
<p>A <span class="math"><em>g</em><em>r</em><em>o</em><em>u</em><em>p</em></span> further constrains a monoid by adding an inverse to all set elements. For example <span class="math">2</span> is the inverse of <span class="math">3</span> in the group (<span class="math"><em>Z</em><sub>5</sub></span>, <span class="math"> + </span>), since <span class="math">2 + 3 = 5 ≡ 0 = <em>e</em></span>.</p>
<p>Naturally negated numbers (as members of <span class="math"><em>Z</em><sub><em>k</em></sub></span>) form candidates for inverses. But it is not really applicable to strings and string concatenation. Groups are however heavily used as building blocks for more advanced constructions, which will not be covered in this post. They are also fundamental in algorithms such as the <a href="https://en.wikipedia.org/wiki/General_number_field_sieve">General Number Field Sieve</a>.</p>
<p>On hackage, a declaration can be found in the <a href="http://hackage.haskell.org/packages/archive/groups/0.3.0.0/doc/html/Data-Group.html">groups</a> package. The typeclass can be given as:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="kw">class</span> <span class="dt">Monoid</span> a <span class="ot">=&gt;</span> <span class="dt">Group</span> a <span class="kw">where</span>
<span class="ot">  finv ::</span> a <span class="ot">-&gt;</span> a</code></pre>
<h4 id="conclusions">Conclusions</h4>
<p>There is an implementation of Fenwick Trees available <a href="https://github.com/davnils/fenwick-semi">here</a>, which identifies and implements a semigroup-based structure. I might write a detalied blog post on it in the future.</p>
<p>The benefits of the algebraic approach can be narrowed down to two points:</p>
<ul>
<li>Writing the implementation in terms of algebraic structures minimizes code repetition and allows trivial generalization.</li>
<li>Working over some structure of a certain type constrains the implementation and eliminates a set of mistakes, such as using specific constants or relying on implicit assumptions in behaviour. This follows since all operations must be well-defined over any instance of the chosen structure.</li>
</ul>
<p>This survey has been quite limited in practice since it only bothers with some basic concepts in algebra. In languages such as Haskell there is also a lot of usage from category theory, where concepts such as categories, functors and monads exist.</p>
<p>There are some practical difficulties since typeclasses are seldom given as expected. For example the base package defines a monoid in isolation, which implies that there is quite some repetition when defining structures. This is also present in the definitions of other classes such as monads. Reality does unfortunately not always meet the theoretic beauty…</p>
<p>For further information I can recommend the book “A First Course In Abstract Algebra” by Fraleigh, and also looking into category theory and different applications in Haskell.</p>

  <div class="box"></div>
</div>


  <a id="reliable_blind_sqli"></a>
  <div class="post">
<h2><a href="#reliable_blind_sqli" id="postTitle">Reliable Blind SQL Injections</a></h2>

<div class="info">
  Posted on June 20, 2013
</div>

  <h4 id="introduction">Introduction</h4>
<p>This post will study the problem of reliably detecting blind SQL injections, and more specifically how statistical methods can be utilized.</p>
<p>First lets begin with defining blind SQL injections. Basically a SQL injection is considered blind when there is no direct result being observed in the reply generated by a server, but it still executes code supplied by an attacker. Hence one option is to resort to observing side-effects, e.g. timing, external connections etc. The focus of this post is to provide more reliable detection of timing-based blind SQL injections.</p>
<p>Typically blind SQL injections are detected by running a query which consumes a lot of time. Such a query might for example be <i>sleep 5000</i>, which would delay the server response in 5 seconds. There are some basic potential issues with this approach:</p>
<ul>
<li>There is an implicit assumption of some deterministic (in the time domain) function, such as <i>sleep</i>.</li>
<li>Large sleep intervals (or heavy computations) will be easy to detect when monitoring a system. After all they would be chosen as to stand out of the typical response time distribution.</li>
<li>Load fluctuations on the server side and networks effect will make it less reliable.</li>
<li>Large offsets may result in server timeouts.</li>
</ul>
<h4 id="statistical-approach">Statistical approach</h4>
<p>Before describing a statistical model of deciding if a blind SQL injection actually exists, I will describe some desirable goals.</p>
<p>First of all, it would be great to make all underlying assumptions clear and describe relationships between different parameters, i.e. having a well-defined problem. It would also be great to generalize the <i>blind SQL</i> part to other databases and different kind of queries.</p>
<p>A basic assumption would be to have a payload executed on the target machine resulting in a <i>significant</i> delay, before it transmits the reply. This is rather vaguely worded but will be heavily dependent on parameters, as presented in the coming section.</p>
<p>Given the ability to submit some passive probe <span class="math"><em>f</em><sub><em>p</em><em>a</em><em>s</em><em>s</em><em>i</em><em>v</em><em>e</em></sub></span> (a normal request) and some active probe <span class="math"><em>f</em><sub><em>a</em><em>c</em><em>t</em><em>i</em><em>v</em><em>e</em></sub></span> (with the potential injection), it is possible to build two distributions of response times. You can think of the distributions as two histograms (or tables) of values. This allows us to incorporate information from several requests into deciding if there actually exists a blind SQL injection. The basic approach is to either confirm the existence of a blind SQL injection or keep on making requests, hence adding more information. The goal would then be to tell if there is a significant deviation between the distributions, a problem which can be handled using statistics.</p>
<h4 id="bootstrap-and-hypothesis-testing">Bootstrap and hypothesis testing</h4>
<p>The section will present the bootstrap and how it can be used to define if two distributions are “equal”, while still not breaking any assumptions.</p>
<p>Given a set of observed values (such as response times of a passive probe) as a vector <span class="math"><em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, . . . , <em>x</em><sub><em>n</em></sub></span>, we would like to define statistical measures on the underlying distribution. This is typically not trivial since a response time distribution might differ from some <span class="math"><em>N</em>(<em>μ</em>, <em>σ</em><sup>2</sup>)</span> distribution, partly since the central limit theorem is not applicable in the general case.</p>
<p>Non-parameterized bootstrap (the version used here) provides a way of defining a proper distribution based on the given set of samples. It is defined as follows:</p>
<ul>
<li>Given some set of samples <span class="math"><em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, . . . , <em>x</em><sub><em>n</em></sub></span> drawn from <span class="math"><em>X</em><sub>1</sub>, <em>X</em><sub>2</sub>, . . . , <em>X</em><sub><em>n</em></sub></span> , the distribution formed by <span class="math"><em>X</em> * <sub>1</sub>, <em>X</em> * <sub>2</sub>, . . . , <em>X</em> * <sub><em>n</em></sub></span> can be estimated by taking B samples on the form <span class="math"><em>x</em> * <sub>1</sub>, <em>x</em> * <sub>2</sub>, . . . , <em>x</em> * <sub><em>n</em></sub></span> from the original set (with replacement), and it converges as <span class="math"><em>B</em> →  + ∞</span> (or when all B possible samples have been taken).</li>
</ul>
<p>Underlying this theorem is the idea of <span class="math"><em>X</em> * <sub>1</sub>, <em>X</em> * <sub>2</sub>, . . . , <em>X</em> * <sub><em>n</em></sub></span> representing the acquired samples. Typically the parameter <span class="math"><em>B</em></span> will not cover all possible sets of samples, hence we are left with an approximation (this is due to to the exponential increase with respect to <span class="math"><em>n</em></span>).</p>
<p>Given this distribution it is possible to evaluate different types of estimators, i.e. different statistical measures. In our case it is of interest to study <span class="math"><em>θ</em>(<em>b</em>) = {<em>f</em><sub><em>p</em><em>a</em><em>s</em><em>s</em><em>i</em><em>v</em><em>e</em></sub>}<sub><em>b</em></sub> − {<em>f</em><sub><em>a</em><em>c</em><em>t</em><em>i</em><em>v</em><em>e</em></sub>}<sub><em>b</em></sub></span>, the difference in arithmetic mean between the <span class="math"><em>b</em></span>’th sample of passive and active probes.</p>
<p>By evaluating <span class="math"><em>θ</em>(<em>b</em>)</span> for <span class="math"><em>b</em> = 1, . . . , <em>B</em></span>, for some chosen <span class="math"><em>B</em></span>, a distribution of difference in arithmetic means is constructed. This distribution can then be used to draw statistically sound conclusions, and construct confidence intervals for these results. A confidence interval can be constructed using the <i>percentile method</i>, which basically corresponds to the lower and upper percentiles of the distribution with respect a significance parameter. In this particular case we would like to survey if there is some significant difference indicating that a blind SQL injection does exist. This corresponds to the null hypothesis <span class="math"><em>H</em><sub>0</sub></span> : ‘There is no blind SQL injection’. It will be evaluated over the given distribution for some significance level parameter <span class="math"><em>α</em></span> (typically 0.05 or 0.01). If the null hypothesis is rejected this implies that a blind SQL injection does exist (at the specified significance level), or else no conclusions can be drawn (at least according to this specific definition).</p>
<h4 id="example-implementation">Example implementation</h4>
<p>A sample implementation exists at <a href="https://github.com/davnils/blind-sqli-stats">blind-sqli-stats</a> which implements bootstrap, the percentile method, and hypothesis testing. All input measurements are read from stdin, which would correspond to an asynchronous HTTP request in a real life implementation. It is constructed in a way that only requires a small initial set of samples from <span class="math">{<em>f</em><sub><em>a</em><em>c</em><em>t</em><em>i</em><em>v</em><em>e</em></sub>}</span> and <span class="math">{<em>f</em><sub><em>p</em><em>a</em><em>s</em><em>s</em><em>i</em><em>v</em><em>e</em></sub>}</span>, and will then expand both sets with one element until the null hypothesis is rejected, or some upper limit in the number of samples is reached.</p>
<p>There are also a few sample distributions included. All of these are normally distributed with mixed classifications (i.e. both significant and non-significant examples), but the actual implementation should be able to handle arbitrary distributions.</p>
<p>An important function is ‘rejected’ which decides if, based on the two sets of samples, the null hypothesis should be rejected. It is implemented as follows:</p>
<pre><code>bool rejected(const std::vector&lt;double&gt; &amp; x,
              const std::vector&lt;double&gt; &amp; y)
{
  std::vector&lt;double&gt; bootStrapped;
  bootStrapped.reserve(BOOTSTRAP_SAMPLES);

  for(auto b = 0U; b &lt; BOOTSTRAP_SAMPLES; b++)
  {
    auto xSample = sample(x, x.size());
    auto ySample = sample(y, y.size());

    bootStrapped.push_back(
      average(xSample) - average(ySample)
      );
  }

  std::sort(std::begin(bootStrapped), std::end(bootStrapped));

  auto bounds = getInterval(bootStrapped, SIGNIFICANCE_ALPHA);

  return(bounds.first &gt; 0 || bounds.second &lt; 0);
}</code></pre>
<p>Basically the two input vectors are sampled <span class="math"><em>B</em></span> times and the difference in arithmetic means is saved for each sample. <span class="math"><em>g</em><em>e</em><em>t</em><em>I</em><em>n</em><em>t</em><em>e</em><em>r</em><em>v</em><em>a</em><em>l</em></span> is applied to retrieve the corresponding bounds of the confidence interval, and if zero is excluded then the null hypothesis can be rejected (at SIGNIFICANCE_ALPHA).</p>
<p>Here is an example run demonstrating the ability to detect significant differences (2*100 samples available with an offset of 0.01 seconds):</p>
<pre><code>d@burk:~/work/sqli-stats$ head -n1 data/normal_5
# N(0.3, 1/30) and N(0.29, 1/30)
d@burk:~/work/sqli-stats$ ./out &lt; data/normal_5 
Null hypothesis rejected: blind sql injection highly likely</code></pre>
<h4 id="discussion">Discussion</h4>
<p>This implementation demonstrates how all assumptions have been made clear and provides reliable decisions. There are however several parameters that likely need to be tuned to fit the needs of specific implementations and use cases. For example the significance level <span class="math"><em>α</em></span> and the number of bootstrap samples <span class="math"><em>B</em></span> should typically be set according to computational constraints, while still maintaining sound statistic measures.</p>
<p>Some other practical issues include choosing the minimum number of samples required from <span class="math">{<em>f</em><sub><em>a</em><em>c</em><em>t</em><em>i</em><em>v</em><em>e</em></sub>}</span> and <span class="math">{<em>f</em><sub><em>p</em><em>a</em><em>s</em><em>s</em><em>i</em><em>v</em><em>e</em></sub>}</span>, and deciding on some upper limit.</p>
<p>Until now all descriptions have been given as the null hypothesis being rejected, or nothing can be concluded. This is of course a naive approach and information regarding probes (e.g. an active probe should result in significant deviation) should allow confirming the non-existence of injections.</p>
<p>I should also mention that the percentile method is very basic and will not render optimal results in all cases. An alternative is the <span class="math"><em>B</em><em>C</em><sub><em>a</em></sub></span> method, which modifies the chosen interval points to account for skew and bias.</p>

  <div class="box"></div>
</div>


  <a id="the_beginning"></a>
  <div class="post">
<h2><a href="#the_beginning" id="postTitle">The Beginning</a></h2>

<div class="info">
  Posted on June 16, 2013
</div>

  <p>This post indicates the beginning of this blog, which will cover various problems and projects, mostly related to theoretic computer science, math, and functional programming.</p>

  <div class="box"></div>
</div>



        </div>
    </body>
</html>
