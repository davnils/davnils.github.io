<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
    <head>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
        <title>blog :: Thought → IO ∅</title>
        <link rel="stylesheet" type="text/css" href="./css/default.css" />

        <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/javascript">
        MathJax.Hub.Config({
          tex2jax: {
            inlineMath: [ ['$','$'], ['\\(','\\)'] ]
          }
        });
        </script>
    </head>
    <body>
        <div id="header">
            <div id="navigation">
                <a href="./">Blog</a>
                <a href="https://github.com/davnils" target="_blank">Code</a>
                <a href="./about.html">About</a>
            </div>
        </div>

        <div id="content">
            <h1>blog :: Thought → IO ∅</h1>
            
  <a href="#Thoughts On Service Fingerprinting"></a>
  <div class="post">
<h2>Thoughts On Service Fingerprinting</h2>

<div class="info">
  Posted on August 18, 2013
</div>

  <p>There are a lot of services connected to the public internet and private networks. A crucial part of pentesting different networks involve learning about what software is running and also more detailed information such as versions. This allows more efficient testing by limiting the number of probable vulnerabilities, but also makes deeper pentesting feasible.</p>
<p>Obviously there are problems when services try to hide their true identity. This might be present in the case of HTTP servers, where the Server field describes the software name and version. In this case the field will typically indicate the correct version, but this simple heuristic will fail in any non-trivial cases, and does not really provide any useful information. Hence a high quality scanner, operating on some sort of services, should be to reliably detect software without depending on these obvious fields. The term fingerprinting typically involves reliably detecting software by studying all available information.</p>
<p>Fundamentally there are different domains of information which can be studied. This post covers the <i>Data</i> and <i>Time</i> domain, both of which applicable to varying degrees with different inherent properties. In both cases it is of interest to classify the software (and perhaps also software version), by choosing the most likely model that fits the data. Hence you will typically need some reference data set from which models of software combinations can be constructed. Given a set of models, the next step is to gather all the possible information, and retrieve the most likely model.</p>
<p>The data domain is what I consider being basically an application-level packet capture of network traffic. For example this is the transaction of HTTP headers captured when making an HTTP request. The benefits of only utilising this information boils down to simplicity and generality. An obvious example of this is that the underlying network stack and transport path does not affect the information content. If fingerprinting is done in a passive manner (e.g. with a provided packet capture) it might not be optimal. In this case the session keys must be provided, assuming that the entire session is encrypted. But it is still very powerful in an active setting which is likely the most common.</p>
<p>Classification in the data domain can be done with a range of different techniques. I will describe using an artificial neural network (ANN) classifier built using standard primitives. Firstly there is the problem of converting a series of headers into a suitable representation that will be fed into the classifier. This example will only consider the header part, even though the response body will certainly contain useful information when issuing certain requests. The series of headers should be transformed into a fixed-dimensional (length) vector in some way. This vector can then be used as input into a multi-level perceptron (a standard ANN) and be trained using supervised learning (i.e. given data set with input and result). Beginning by stating that the first k headers should be preserved for learning (with some large enough k), this limits the maximum vector length. Each header is on the format “key: value” where “key” belongs to a set of possible keys and “value” may be an arbitrary string or integer. Converting these into our fixed dimensional space imposes some natural restrictions, e.g. take the k first bytes and quantize integer values into ranges of interest. There is also the question of ordering - should headers be sorted according to some predefined ordering? This will obviously limit classification based on ordering properties and should probably not be used when not part of the standard.</p>
<p>Labeling all the keys (quantization) can be done by evaluating them over an autoencoder ANN. This is constructed as a three-layer perceptron where the input and output layers are identical (when learning is complete). The inner layer is smaller and represents compressed versions of all the inputs. Hence when learning is complete (i.e. inputs and outputs match for all labels), the first two layers can be extracted without losing any information. The output given by the inner layer is then used to construct label representation in the header vector, which in turn is used as input in the classifier. Note that training is simply repeated with a smaller inner layer until the model can’t describe all of the information content anymore.</p>
<p>Given a data set with a set of headers and their corresponding reference services, all the inputs can be built. Typically the fixed dimensional vector will be chunked into fixed length pieces, each corresponding to a row in the complete header packet. Each header row is then labeled as discussed and paired with a fixed length value field. This is a one-time process (including the autoencoder step) and will produce a processed data set that is fit for further real classification, independent of the method being employed. In this case the multi-layer perceptron can be trained using the backprop algorithm. The resulting classifier can then be used in practice by performing the “input processing” step for a data set of unknown classification, resulting in the most likely software.</p>
<p>The time domain provides a different set of properties but is typically not as generally applicable as classification in the data domain. Lets view HTTP requests from a different perspective; essentially the client makes a request and receives a reply which can be measured as bytes over time, possibly with some meaning associated to chunks of bytes. It is reasonable to believe that different services might produce different amounts of content over time, after all such a concern is not really present in the mind of an average developer. Simply plotting the number of bytes retrieved over time will yield a graph which could be studied manually in order to discern any interesting properties of a specific service. Automatic classification is obviously also interesting in this domain. There have been work on <a href="http://www.cs.ucr.edu/~eamonn/time_series_data/">time series classification</a> which will take the complete data series and perform a classification. Alternatively it is possible to view the underlying service as something stateful. Consider for example that a XYZ service might deliver data through a set of different stages, perhaps performing some initial throttling of bandwidth and loosening up later on. Such features can be detected by taking the observations and comparing them with a set of existing models, extracting the most likely one. In the more general case it is common to use Hidden Markov Models (HMM) which are capable of deducing facts about the internal states by simply performing a chain of observations. Some very similar work has been done by <a href="http://events.ccc.de/congress/2011/Fahrplan/events/4732.en.html">Burschka</a> (originally presented at the sec-t conference) where encrypted Skype traffic was monitored and the time domain was classified in order to detect most likely spoken phrases, without access to the underlying audio data.</p>
<p>There are however some potential issues when dealing with time domain classification. Commonly such an attack would be performed across LANs in a protocol encapsulated with TCP/IP. This interferes with timing in several ways, due to things like IP fragmentation and TCP segmentation, tightly bonded with MSS and congestion control. Obviously there is also the problem of variance over long network paths. Reducing the variance requires relocating probes or working around the problem with a larger set of samples, potentially becoming infeasible in a practical context.</p>
<p>In the end it is probably not feasible to perform classification in the time domain across networks. It might however be practical in certain contexts, e.g. in the case of a reduced network stack or very low latency network links with deterministic stacks. The data domain does however provide a reasonable set of assumptions and could be used to perform reliable classification for a wide range of services, at least as long as there is no active interference beyond the trivial cases. Using supervised learning to build a classifier does however require plenty of effort in acquiring a reliable data set over a wide range of configurations. Perhaps this could be improved by performing unsupervised learning while labeling the corresponding output classes in some clever way.</p>
</div>

<div class="box"></div>


  <a href="#Algebra and Information Structure"></a>
  <div class="post">
<h2>Algebra and Information Structure</h2>

<div class="info">
  Posted on July  2, 2013
</div>

  <p>Today I am writing on how it’s possible to find structure in information, bundled with functions, and how this can be applied in practice. This is highly applicable to software development but also comes from an interesting theoretical background.</p>
<p>So, lets begin with information. What can be said about a data set sharing some properties? Is it possible to relate some common operations and make general statements? Lets say we define a data structure working over integers - can it be extended to arbitrary objects forming some property? It would be nice if we could write general algorithms and data structures, with the fundamental constraints in mind, and reuse them to the fullest possible extent.</p>
<p>Algebra is a mathematical field which discusses, amongst other things, sets and related functions. This allows us to encapsulate information by providing a set of rules that need to be satisfied, in order for the information and possible functions to belong to some abstraction. In this post a basic set of abstractions are described and some practical examples are given. These are bundled with corresponding definitions in Haskell code, but the reader does not need any previous knowledge of the language in order to grasp the essence of these constructs.</p>
<h4 id="algebra">Algebra</h4>
<p>Lets begin with some definitions required to discuss these topics. Given a binary function <span class="math"><em>f</em>(<em>a</em>, <em>a</em>)</span> (denoted inline <span class="math"> † </span>) over a set <span class="math"><em>S</em></span>, the following is of interest (for all <span class="math"><em>a</em>, <em>b</em>, <em>c</em> ∈ <em>S</em></span>):</p>
<ul>
<li>function associativity: <span class="math">(<em>a</em> † <em>b</em>) † <em>c</em> = <em>a</em> † (<em>b</em> † <em>c</em>)</span></li>
<li>identity element <span class="math"><em>e</em></span> of <span class="math"><em>S</em></span>: <span class="math"><em>a</em> † <em>e</em> = <em>e</em> † <em>a</em> = <em>a</em></span></li>
<li>inverse element <span class="math"><em>a</em><sup> − 1</sup> ∈ <em>S</em></span> of <span class="math"><em>a</em> ∈ <em>S</em></span>: <span class="math"><em>a</em> † <em>a</em><sup> − 1</sup> = <em>a</em><sup> − 1</sup> † <em>a</em> = <em>e</em></span></li>
</ul>
<p>Furthermore <span class="math"><em>f</em></span> is <span class="math"><em>c</em><em>l</em><em>o</em><em>s</em><em>e</em><em>d</em></span> over <span class="math"><em>S</em></span> if <span class="math"><em>f</em>(<em>a</em>, <em>b</em>) ∈ <em>S</em></span></p>
<p>Adding these properties will make the function and set more capable of expressing different algorithms. But it also constrains the possible implementations (e.g. identity might not always exists), hence it is interesting to choose the smallest possible subset of laws that need to be satisfied.</p>
<p>Lets begin with the basic structures and gradually add more constrains, while studying applications of each subset.</p>
<h5 id="magma">Magma</h5>
<p>The simplest possible definition that is covered is the <span class="math"><em>m</em><em>a</em><em>g</em><em>m</em><em>a</em></span>. It consist of a binary operation <span class="math"><em>f</em></span> and a set <span class="math"><em>S</em></span>. The only requirement is that <span class="math"><em>f</em></span> is closed over <span class="math"><em>S</em></span>, nothing more.</p>
<p>Of course this very simplistic definition does not yield any structure in the problem and anything more constrained will still be considered a valid magma. It can modeled in Haskell using a type class on the following format:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="kw">class</span> <span class="dt">Magma</span> a <span class="kw">where</span>
<span class="ot">  f ::</span> a <span class="ot">-&gt;</span> a <span class="ot">-&gt;</span> a</code></pre>
<p>Hence any implementation (declared with instance DataType where …) would implement the function. It can be found in the <a href="http://hackage.haskell.org/package/magma">magma package</a> available on hackage.</p>
<h5 id="semigroup">Semigroup</h5>
<p>Semigroups add the associativity property to the magma. The name is derived from being a generalized version of groups, which are described below. Associativity allows a lot more powerful manipulations, which is appearant in the corresponding hackage package <a href="http://hackage.haskell.org/package/semigroups">semigroups</a>.</p>
<p>Due to the relatively weak typing being applied here there is no difference to the magma-typeclass, except with the name being substituted. Hence associativity is implicit on the type level, at least in this article.</p>
<p><span class="math"><em>F</em><em>o</em><em>l</em><em>d</em><em>i</em><em>n</em><em>g</em></span> is the operation of transforming a finite set into a single scalar value. Typically this can be realised by simply applying <span class="math"><em>f</em></span> onto the set repeatedly. This follows since the set cardinality is reduced by one for each application. Singleton sets can be considered as already reduced or invalid, the same applies to <span class="math">$\varnothing$</span>.</p>
<p>Without associativity this must be done while maintaining order and evaluating each operation in some precise way. Associativity allows us to simply study the set of values and apply the reduction in some arbitrary order. For example it is easy to realise a parallel reduction in a tree-like fashion. Simply by taking pairs of elements from the original set and transforming each pair in parallel to a scalar, this can be repeated until the final value is obtained. The final time complexity becomes <span class="math">$O(\log \left\vert S \right\vert)$</span> given the assumption that constructing <span class="math">{∀ <em>a</em> ∈ <em>S</em>: <em>f</em>(<em>a</em>)}</span> is considered <span class="math"><em>O</em>(1)</span> time, e.g. with a large amount of cores.</p>
<p>A simple example of folding is summing up a set of numbers, or taking the product. In the semigroups package there is also a nice application of non-empty lists being viewed as semigroups.</p>
<h5 id="monoid">Monoid</h5>
<p>A <span class="math"><em>m</em><em>o</em><em>n</em><em>o</em><em>i</em><em>d</em></span> is a semigroup that is constrained by satsifying the identity property. It’s typically used in a lot of different constructs and is present in the standard Haskell library (<a href="http://www.haskell.org/ghc/docs/latest/html/libraries/base/Data-Monoid.html">the base package</a>). It can be modelled with the following type class, where pure denotes the identity element:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="kw">class</span> <span class="dt">Semigroup</span> a <span class="ot">=&gt;</span> <span class="dt">Monoid</span> a <span class="kw">where</span>
<span class="ot">  pure ::</span> a</code></pre>
<p>The identity property allows trivial folding in the general case since we can always treat singleton sets <span class="math">{<em>a</em>}</span> or <span class="math">$\varnothing$</span> by using the identity element, as <span class="math"><em>f</em>(<em>a</em>, <em>e</em>)</span> and <span class="math"><em>e</em></span> respectively.</p>
<p>Monoids are extensively used within several fields and form a heavily utilized Haskell idiom when writing generic code. An example instance is string-like structures which form a monoid as follows:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="kw">instance</span> <span class="dt">Semigroup</span> <span class="dt">String</span> <span class="kw">where</span>
  f <span class="fu">=</span> <span class="fu">concat</span>

<span class="kw">instance</span> <span class="dt">Monoid</span> <span class="dt">String</span> <span class="kw">where</span>
  pure <span class="fu">=</span> <span class="st">&quot;&quot;</span></code></pre>
<p>This allows easily swapping the underlying string representation if only occurrences of <span class="math"><em>f</em></span> are used when performing string concatenation. There are also more advanced applications such as <a href="http://izbicki.me/blog/hlearn-cross-validates-400x-faster-than-weka">statistics</a>.</p>
<h5 id="group">Group</h5>
<p>A <span class="math"><em>g</em><em>r</em><em>o</em><em>u</em><em>p</em></span> further constrains a monoid by adding an inverse to all set elements. For example <span class="math">2</span> is the inverse of <span class="math">3</span> in the group (<span class="math"><em>Z</em><sub>5</sub></span>, <span class="math"> + </span>), since <span class="math">2 + 3 = 5 ≡ 0 = <em>e</em></span>.</p>
<p>Naturally negated numbers (as members of <span class="math"><em>Z</em><sub><em>k</em></sub></span>) form candidates for inverses. But it is not really applicable to strings and string concatenation. Groups are however heavily used as building blocks for more advanced constructions, which will not be covered in this post. They are also fundamental in algorithms such as the <a href="https://en.wikipedia.org/wiki/General_number_field_sieve">General Number Field Sieve</a>.</p>
<p>On hackage, a declaration can be found in the <a href="http://hackage.haskell.org/packages/archive/groups/0.3.0.0/doc/html/Data-Group.html">groups</a> package. The typeclass can be given as:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="kw">class</span> <span class="dt">Monoid</span> a <span class="ot">=&gt;</span> <span class="dt">Group</span> a <span class="kw">where</span>
<span class="ot">  finv ::</span> a <span class="ot">-&gt;</span> a</code></pre>
<h4 id="conclusions">Conclusions</h4>
<p>There is an implementation of Fenwick Trees available <a href="https://github.com/davnils/fenwick-semi">here</a>, which identifies and implements a semigroup-based structure. I might write a detalied blog post on it in the future.</p>
<p>The benefits of the algebraic approach can be narrowed down to two points:</p>
<ul>
<li>Writing the implementation in terms of algebraic structures minimizes code repetition and allows trivial generalization.</li>
<li>Working over some structure of a certain type constrains the implementation and eliminates a set of mistakes, such as using specific constants or relying on implicit assumptions in behaviour. This follows since all operations must be well-defined over any instance of the chosen structure.</li>
</ul>
<p>This survey has been quite limited in practice since it only bothers with some basic concepts in algebra. In languages such as Haskell there is also a lot of usage from category theory, where concepts such as categories, functors and monads exist.</p>
<p>There are some practical difficulties since typeclasses are seldom given as expected. For example the base package defines a monoid in isolation, which implies that there is quite some repetition when defining structures. This is also present in the definitions of other classes such as monads. Reality does unfortunately not always meet the theoretic beauty…</p>
<p>For further information I can recommend the book “A First Course In Abstract Algebra” by Fraleigh, and also looking into category theory and different applications in Haskell.</p>
</div>

<div class="box"></div>


  <a href="#Reliable Blind SQL Injections"></a>
  <div class="post">
<h2>Reliable Blind SQL Injections</h2>

<div class="info">
  Posted on June 20, 2013
</div>

  <h4 id="introduction">Introduction</h4>
<p>This post will study the problem of reliably detecting blind SQL injections, and more specifically how statistical methods can be utilized.</p>
<p>First lets begin with defining blind SQL injections. Basically a SQL injection is considered blind when there is no direct result being observed in the reply generated by a server, but it still executes code supplied by an attacker. Hence one option is to resort to observing side-effects, e.g. timing, external connections etc. The focus of this post is to provide more reliable detection of timing-based blind SQL injections.</p>
<p>Typically blind SQL injections are detected by running a query which consumes a lot of time. Such a query might for example be <i>sleep 5000</i>, which would delay the server response in 5 seconds. There are some basic potential issues with this approach:</p>
<ul>
<li>There is an implicit assumption of some deterministic (in the time domain) function, such as <i>sleep</i>.</li>
<li>Large sleep intervals (or heavy computations) will be easy to detect when monitoring a system. After all they would be chosen as to stand out of the typical response time distribution.</li>
<li>Load fluctuations on the server side and networks effect will make it less reliable.</li>
<li>Large offsets may result in server timeouts.</li>
</ul>
<h4 id="statistical-approach">Statistical approach</h4>
<p>Before describing a statistical model of deciding if a blind SQL injection actually exists, I will describe some desirable goals.</p>
<p>First of all, it would be great to make all underlying assumptions clear and describe relationships between different parameters, i.e. having a well-defined problem. It would also be great to generalize the <i>blind SQL</i> part to other databases and different kind of queries.</p>
<p>A basic assumption would be to have a payload executed on the target machine resulting in a <i>significant</i> delay, before it transmits the reply. This is rather vaguely worded but will be heavily dependent on parameters, as presented in the coming section.</p>
<p>Given the ability to submit some passive probe <span class="math"><em>f</em><sub><em>p</em><em>a</em><em>s</em><em>s</em><em>i</em><em>v</em><em>e</em></sub></span> (a normal request) and some active probe <span class="math"><em>f</em><sub><em>a</em><em>c</em><em>t</em><em>i</em><em>v</em><em>e</em></sub></span> (with the potential injection), it is possible to build two distributions of response times. You can think of the distributions as two histograms (or tables) of values. This allows us to incorporate information from several requests into deciding if there actually exists a blind SQL injection. The basic approach is to either confirm the existence of a blind SQL injection or keep on making requests, hence adding more information. The goal would then be to tell if there is a significant deviation between the distributions, a problem which can be handled using statistics.</p>
<h4 id="bootstrap-and-hypothesis-testing">Bootstrap and hypothesis testing</h4>
<p>The section will present the bootstrap and how it can be used to define if two distributions are “equal”, while still not breaking any assumptions.</p>
<p>Given a set of observed values (such as response times of a passive probe) as a vector <span class="math"><em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, . . . , <em>x</em><sub><em>n</em></sub></span>, we would like to define statistical measures on the underlying distribution. This is typically not trivial since a response time distribution might differ from some <span class="math"><em>N</em>(<em>μ</em>, <em>σ</em><sup>2</sup>)</span> distribution, partly since the central limit theorem is not applicable in the general case.</p>
<p>Non-parameterized bootstrap (the version used here) provides a way of defining a proper distribution based on the given set of samples. It is defined as follows:</p>
<ul>
<li>Given some set of samples <span class="math"><em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, . . . , <em>x</em><sub><em>n</em></sub></span> drawn from <span class="math"><em>X</em><sub>1</sub>, <em>X</em><sub>2</sub>, . . . , <em>X</em><sub><em>n</em></sub></span> , the distribution formed by <span class="math"><em>X</em> * <sub>1</sub>, <em>X</em> * <sub>2</sub>, . . . , <em>X</em> * <sub><em>n</em></sub></span> can be estimated by taking B samples on the form <span class="math"><em>x</em> * <sub>1</sub>, <em>x</em> * <sub>2</sub>, . . . , <em>x</em> * <sub><em>n</em></sub></span> from the original set (with replacement), and it converges as <span class="math"><em>B</em> →  + ∞</span> (or when all B possible samples have been taken).</li>
</ul>
<p>Underlying this theorem is the idea of <span class="math"><em>X</em> * <sub>1</sub>, <em>X</em> * <sub>2</sub>, . . . , <em>X</em> * <sub><em>n</em></sub></span> representing the acquired samples. Typically the parameter <span class="math"><em>B</em></span> will not cover all possible sets of samples, hence we are left with an approximation (this is due to to the exponential increase with respect to <span class="math"><em>n</em></span>).</p>
<p>Given this distribution it is possible to evaluate different types of estimators, i.e. different statistical measures. In our case it is of interest to study <span class="math"><em>θ</em>(<em>b</em>) = {<em>f</em><sub><em>p</em><em>a</em><em>s</em><em>s</em><em>i</em><em>v</em><em>e</em></sub>}<sub><em>b</em></sub> − {<em>f</em><sub><em>a</em><em>c</em><em>t</em><em>i</em><em>v</em><em>e</em></sub>}<sub><em>b</em></sub></span>, the difference in arithmetic mean between the <span class="math"><em>b</em></span>’th sample of passive and active probes.</p>
<p>By evaluating <span class="math"><em>θ</em>(<em>b</em>)</span> for <span class="math"><em>b</em> = 1, . . . , <em>B</em></span>, for some chosen <span class="math"><em>B</em></span>, a distribution of difference in arithmetic means is constructed. This distribution can then be used to draw statistically sound conclusions, and construct confidence intervals for these results. A confidence interval can be constructed using the <i>percentile method</i>, which basically corresponds to the lower and upper percentiles of the distribution with respect a significance parameter. In this particular case we would like to survey if there is some significant difference indicating that a blind SQL injection does exist. This corresponds to the null hypothesis <span class="math"><em>H</em><sub>0</sub></span> : ‘There is no blind SQL injection’. It will be evaluated over the given distribution for some significance level parameter <span class="math"><em>α</em></span> (typically 0.05 or 0.01). If the null hypothesis is rejected this implies that a blind SQL injection does exist (at the specified significance level), or else no conclusions can be drawn (at least according to this specific definition).</p>
<h4 id="example-implementation">Example implementation</h4>
<p>A sample implementation exists at <a href="https://github.com/davnils/blind-sqli-stats">blind-sqli-stats</a> which implements bootstrap, the percentile method, and hypothesis testing. All input measurements are read from stdin, which would correspond to an asynchronous HTTP request in a real life implementation. It is constructed in a way that only requires a small initial set of samples from <span class="math">{<em>f</em><sub><em>a</em><em>c</em><em>t</em><em>i</em><em>v</em><em>e</em></sub>}</span> and <span class="math">{<em>f</em><sub><em>p</em><em>a</em><em>s</em><em>s</em><em>i</em><em>v</em><em>e</em></sub>}</span>, and will then expand both sets with one element until the null hypothesis is rejected, or some upper limit in the number of samples is reached.</p>
<p>There are also a few sample distributions included. All of these are normally distributed with mixed classifications (i.e. both significant and non-significant examples), but the actual implementation should be able to handle arbitrary distributions.</p>
<p>An important function is ‘rejected’ which decides if, based on the two sets of samples, the null hypothesis should be rejected. It is implemented as follows:</p>
<pre><code>bool rejected(const std::vector&lt;double&gt; &amp; x,
              const std::vector&lt;double&gt; &amp; y)
{
  std::vector&lt;double&gt; bootStrapped;
  bootStrapped.reserve(BOOTSTRAP_SAMPLES);

  for(auto b = 0U; b &lt; BOOTSTRAP_SAMPLES; b++)
  {
    auto xSample = sample(x, x.size());
    auto ySample = sample(y, y.size());

    bootStrapped.push_back(
      average(xSample) - average(ySample)
      );
  }

  std::sort(std::begin(bootStrapped), std::end(bootStrapped));

  auto bounds = getInterval(bootStrapped, SIGNIFICANCE_ALPHA);

  return(bounds.first &gt; 0 || bounds.second &lt; 0);
}</code></pre>
<p>Basically the two input vectors are sampled <span class="math"><em>B</em></span> times and the difference in arithmetic means is saved for each sample. <span class="math"><em>g</em><em>e</em><em>t</em><em>I</em><em>n</em><em>t</em><em>e</em><em>r</em><em>v</em><em>a</em><em>l</em></span> is applied to retrieve the corresponding bounds of the confidence interval, and if zero is excluded then the null hypothesis can be rejected (at SIGNIFICANCE_ALPHA).</p>
<p>Here is an example run demonstrating the ability to detect significant differences (2*100 samples available with an offset of 0.01 seconds):</p>
<pre><code>d@burk:~/work/sqli-stats$ head -n1 data/normal_5
# N(0.3, 1/30) and N(0.29, 1/30)
d@burk:~/work/sqli-stats$ ./out &lt; data/normal_5 
Null hypothesis rejected: blind sql injection highly likely</code></pre>
<h4 id="discussion">Discussion</h4>
<p>This implementation demonstrates how all assumptions have been made clear and provides reliable decisions. There are however several parameters that likely need to be tuned to fit the needs of specific implementations and use cases. For example the significance level <span class="math"><em>α</em></span> and the number of bootstrap samples <span class="math"><em>B</em></span> should typically be set according to computational constraints, while still maintaining sound statistic measures.</p>
<p>Some other practical issues include choosing the minimum number of samples required from <span class="math">{<em>f</em><sub><em>a</em><em>c</em><em>t</em><em>i</em><em>v</em><em>e</em></sub>}</span> and <span class="math">{<em>f</em><sub><em>p</em><em>a</em><em>s</em><em>s</em><em>i</em><em>v</em><em>e</em></sub>}</span>, and deciding on some upper limit.</p>
<p>Until now all descriptions have been given as the null hypothesis being rejected, or nothing can be concluded. This is of course a naive approach and information regarding probes (e.g. an active probe should result in significant deviation) should allow confirming the non-existence of injections.</p>
<p>I should also mention that the percentile method is very basic and will not render optimal results in all cases. An alternative is the <span class="math"><em>B</em><em>C</em><sub><em>a</em></sub></span> method, which modifies the chosen interval points to account for skew and bias.</p>
</div>

<div class="box"></div>


  <a href="#The Beginning"></a>
  <div class="post">
<h2>The Beginning</h2>

<div class="info">
  Posted on June 16, 2013
</div>

  <p>This post indicates the beginning of this blog, which will cover various problems and projects, mostly related to theoretic computer science, math, and functional programming.</p>
</div>

<div class="box"></div>



        </div>
    </body>
</html>
